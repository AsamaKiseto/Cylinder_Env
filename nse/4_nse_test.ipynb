{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../env\")\n",
    "\n",
    "from env.Cylinder_Rotation_Env import Cylinder_Rotation_Env\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from tqdm import tqdm, trange\n",
    "from fenics import * \n",
    "from timeit import default_timer\n",
    "\n",
    "from models import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch-py3.8/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling FFC just-in-time (JIT) compiler, this may take some time.\n",
      "Calling FFC just-in-time (JIT) compiler, this may take some time.\n",
      "Calling FFC just-in-time (JIT) compiler, this may take some time.\n",
      "Calling FFC just-in-time (JIT) compiler, this may take some time.\n",
      "Calling FFC just-in-time (JIT) compiler, this may take some time.\n",
      "Calling FFC just-in-time (JIT) compiler, this may take some time.\n",
      "Calling FFC just-in-time (JIT) compiler, this may take some time.\n",
      "Calling FFC just-in-time (JIT) compiler, this may take some time.\n",
      "Calling FFC just-in-time (JIT) compiler, this may take some time.\n",
      "Calling FFC just-in-time (JIT) compiler, this may take some time.\n",
      "Calling FFC just-in-time (JIT) compiler, this may take some time.\n",
      "start init_solve\n",
      "end init_solve\n",
      "{'dtr': 0.1, 'T': 1, 'rho_0': 1, 'mu': 0.001, 'traj_max_T': 20, 'dimx': 128, 'dimy': 64, 'min_x': 0, 'max_x': 2.2, 'min_y': 0, 'max_y': 0.41, 'r': 0.05, 'center': (0.2, 0.2), 'min_w': -1, 'max_w': 1, 'min_velocity': -1, 'max_velocity': 1, 'U_max': 1.5}\n"
     ]
    }
   ],
   "source": [
    "# env init\n",
    "env = Cylinder_Rotation_Env(params={'dtr': 0.1, 'T': 1, 'rho_0': 1, 'mu' : 1/1000,\n",
    "                                    'traj_max_T': 20, 'dimx': 128, 'dimy': 64,\n",
    "                                    'min_x' : 0,  'max_x' : 2.2, \n",
    "                                    'min_y' : 0,  'max_y' : 0.41, \n",
    "                                    'r' : 0.05,  'center':(0.2, 0.2),\n",
    "                                    'min_w': -1, 'max_w': 1,\n",
    "                                    'min_velocity': -1, 'max_velocity': 1,\n",
    "                                    'U_max': 1.5, })\n",
    "\n",
    "print(env.params)\n",
    "\n",
    "# one step: 10 timestamps for dT = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dT: 1, ny: 64, nx: 128\n"
     ]
    }
   ],
   "source": [
    "# env params\n",
    "dT = env.params['T']\n",
    "nx = env.params['dimx']\n",
    "ny = env.params['dimy']\n",
    "print('dT: {}, ny: {}, nx: {}'.format(dT, ny, nx))\n",
    "\n",
    "# mosel params setting\n",
    "L = 4\n",
    "modes = 12\n",
    "width = 20\n",
    "\n",
    "# data path\n",
    "data_path = './data/nse_data_N0_25_nT_10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Params of phase 1 training\n",
      "epochs: 500\n",
      "batch size: 20\n",
      "learning rate: 0.01\n",
      "weight decay: 0.0001\n",
      "step size:100\n",
      "gamma: 0.5\n"
     ]
    }
   ],
   "source": [
    "# setting of text, train, model\n",
    "\n",
    "# output text\n",
    "ftext = open('./logs/nse_operator_fno_test.txt', 'a', encoding='utf-8')\n",
    "\n",
    "# param setting\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 20\n",
    "lr = 1e-2\n",
    "wd = 1e-4\n",
    "step_size = 100\n",
    "gamma = 0.5\n",
    "# weight = args.weight\n",
    "\n",
    "lambda1 = 1\n",
    "lambda2 = 1\n",
    "lambda3 = 1\n",
    "\n",
    "fname = './logs/nse_operator_fno_test'\n",
    "    \n",
    "# model setting\n",
    "model = FNO_ensemble(modes, modes, width, L).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "print('# Params of phase 1 training')\n",
    "print(f'epochs: {epochs}\\nbatch size: {batch_size}\\nlearning rate: {lr}\\nweight decay: {wd}\\nstep size:{step_size}\\ngamma: {gamma}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Data size\n",
      "N0: 25, nt: 99, ny: 128, nx: 64\n"
     ]
    }
   ],
   "source": [
    "# load data and set params of data\n",
    "\n",
    "# load data\n",
    "data, _, Cd, Cl, ang_vel = torch.load(data_path)\n",
    "Cd = Cd[:, 1:]\n",
    "Cl = Cl[:, 1:]\n",
    "ang_vel = ang_vel[:, 1:]\n",
    "\n",
    "# data param\n",
    "ny = data.shape[2] \n",
    "nx = data.shape[3]\n",
    "s = data.shape[2] * data.shape[3]     # ny * nx\n",
    "N0 = data.shape[0]                    # num of data sets\n",
    "nt = data.shape[1] - 1             # nt\n",
    "Ndata = N0 * nt\n",
    "\n",
    "print('# Data size')\n",
    "print('N0: {}, nt: {}, ny: {}, nx: {}'.format(N0, nt, ny, nx))\n",
    "\n",
    "class NSE_Dataset(Dataset):\n",
    "    def __init__(self, data, Cd, Cl, ang_vel):\n",
    "        Cd = Cd.reshape(N0, nt, 1, 1, 1).repeat([1, 1, ny, nx, 1]).reshape(-1, ny, nx, 1)\n",
    "        Cl = Cl.reshape(N0, nt, 1, 1, 1).repeat([1, 1, ny, nx, 1]).reshape(-1, ny, nx, 1)\n",
    "        ang_vel = ang_vel.reshape(N0, nt, 1, 1, 1).repeat([1, 1, ny, nx, 1]).reshape(-1, ny, nx, 1)\n",
    "        input_data = data[:, :-1].reshape(-1, ny, nx, 3)\n",
    "        output_data = data[:, 1:].reshape(-1, ny, nx, 3)\n",
    "\n",
    "        self.input_data = torch.cat((input_data, ang_vel), dim=-1)\n",
    "        self.output_data = torch.cat((output_data, Cd, Cl), dim=-1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return Ndata\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(self.input_data[idx])\n",
    "        y = torch.FloatTensor(self.output_data[idx])\n",
    "        return x, y\n",
    "\n",
    "NSE_data = NSE_Dataset(data, Cd, Cl, ang_vel)\n",
    "train_data, test_data = random_split(NSE_data, [int(0.8 * Ndata), int(0.2 * Ndata)])\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 460 | (train) loss1: 2.0008e-03,  loss2: 8.0482e-03,  loss3: 8.3562e-01 | (test) loss1: 2.1353e-03, loss2: 1.6181e-02,  loss3: 7.7791e-01:  92%|█████████▏| 460/500 [14:00<01:06,  1.66s/it]"
     ]
    }
   ],
   "source": [
    "# train phase 1\n",
    "pbar = tqdm(total=epochs, file=sys.stdout)\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    \n",
    "    t1 = default_timer()\n",
    "    train_loss = AverageMeter()\n",
    "    train_loss1 = AverageMeter()\n",
    "    train_loss2 = AverageMeter()\n",
    "    train_loss3 = AverageMeter()\n",
    "    train_loss4 = AverageMeter()\n",
    "    test_loss = AverageMeter()\n",
    "    test_loss1 = AverageMeter()\n",
    "    test_loss2 = AverageMeter()\n",
    "    test_loss3 = AverageMeter()\n",
    "    test_loss4 = AverageMeter()\n",
    "\n",
    "    for x_train, y_train in train_loader:\n",
    "        x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # split data read in train_loader\n",
    "        in_train, f_train = x_train[:, :, :, :3], x_train[:, 0, 0, 3]\n",
    "        out_train, Cd_train, Cl_train = y_train[:, :, :, :3], y_train[:, 0, 0, 3], y_train[:, 0, 0, 4]\n",
    "        # put data into model\n",
    "        pred, x_rec, f_rec, trans_out = model(in_train, f_train)\n",
    "        out_latent = model.stat_en(out_train)\n",
    "        in_rec = x_rec[:, :, :, :3]\n",
    "        # prediction items\n",
    "        out_pred = pred[:, :, :, :3]\n",
    "        Cd_pred = torch.mean(pred[:, :, :, 3].reshape(batch_size, -1), 1)\n",
    "        Cl_pred = torch.mean(pred[:, :, :, 4].reshape(batch_size, -1), 1)\n",
    "\n",
    "        # loss1: prediction loss; loss2: rec loss of state\n",
    "        # loss3: rec loss of f; loss4: latent loss\n",
    "        loss1 = F.mse_loss(out_pred, out_train, reduction='mean')\\\n",
    "                + F.mse_loss(Cd_pred, Cd_train, reduction='mean') \\\n",
    "                + F.mse_loss(Cl_pred, Cl_train, reduction='mean')\n",
    "        loss2 = F.mse_loss(in_train, in_rec, reduction='mean')\n",
    "        loss3 = F.mse_loss(f_train, f_rec, reduction='mean')\n",
    "        loss4 = F.mse_loss(out_latent, trans_out, reduction='mean')\n",
    "        loss = loss1 + lambda1 * loss2 + lambda2 * loss3 + lambda3 * loss4\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.update(loss.item(), x_train.shape[0])\n",
    "        train_loss1.update(loss1.item(), x_train.shape[0])\n",
    "        train_loss2.update(loss2.item(), x_train.shape[0])\n",
    "        train_loss3.update(loss3.item(), x_train.shape[0])\n",
    "        train_loss4.update(loss3.item(), x_train.shape[0])\n",
    "    \n",
    "    scheduler.step()\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_test, y_test in test_loader:\n",
    "            x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "\n",
    "            # split data read in test_loader\n",
    "            in_test, f_test = x_test[:, :, :, :3], x_test[:, 0, 0, 3]\n",
    "            out_test, Cd_test, Cl_test = y_test[:, :, :, :3], y_test[:, 0, 0, 3], y_test[:, 0, 0, 4]\n",
    "            # put data into model\n",
    "            pred, x_rec, f_rec, trans_out = model(in_test, f_test)\n",
    "            out_latent = model.stat_en(out_test)\n",
    "            in_rec = x_rec[:, :, :, :3]\n",
    "            # prediction items\n",
    "            out_pred = pred[:, :, :, :3]\n",
    "            Cd_pred = torch.mean(pred[:, :, :, 3].reshape(batch_size, -1), 1)\n",
    "            Cl_pred = torch.mean(pred[:, :, :, 4].reshape(batch_size, -1), 1)\n",
    "            loss1 = F.mse_loss(out_pred, out_test, reduction='mean')\\\n",
    "                    + F.mse_loss(Cd_pred, Cd_test, reduction='mean') \\\n",
    "                    + F.mse_loss(Cl_pred, Cl_test, reduction='mean')\n",
    "            loss2 = F.mse_loss(in_test, in_rec, reduction='mean')\n",
    "            loss3 = F.mse_loss(f_test, f_rec, reduction='mean')\n",
    "            loss4 = F.mse_loss(out_latent, trans_out, reduction='mean')\n",
    "            loss = loss1 + lambda1 * loss2 + lambda2 * loss3 + lambda3 * loss4\n",
    "\n",
    "            test_loss.update(loss.item(), x_test.shape[0])\n",
    "            test_loss1.update(loss1.item(), x_test.shape[0])\n",
    "            test_loss2.update(loss2.item(), x_test.shape[0])\n",
    "            test_loss3.update(loss3.item(), x_test.shape[0])\n",
    "            test_loss4.update(loss3.item(), x_test.shape[0])\n",
    "        \n",
    "    t2 = default_timer()\n",
    "\n",
    "    ftext.write('# {} | (train) loss1: {:1.2e}  loss2: {:1.2e}  loss3: {:1.2e} loss4: {:1.2e} | (test) loss1: {:1.2e} loss2: {:1.2e}  loss3: {:1.2e} loss4: {:1.2e}\\n'\n",
    "                .format(epoch, train_loss1.avg, train_loss2.avg, train_loss3.avg, train_loss4.avg, test_loss1.avg, test_loss2.avg, test_loss3.avg, test_loss4.avg))\n",
    "    \n",
    "    end = '\\r'\n",
    "    pbar.set_description('# {} | (train) loss1: {:1.2e}  loss2: {:1.2e}  loss3: {:1.2e} loss4: {:1.2e} | (test) loss1: {:1.2e} loss2: {:1.2e}  loss3: {:1.2e} loss4: {:1.2e}\\n'\n",
    "                            .format(epoch, train_loss1.avg, train_loss2.avg, train_loss3.avg, train_loss4.avg, test_loss1.avg, test_loss2.avg, test_loss3.avg, test_loss4.avg))\n",
    "    pbar.update()\n",
    "    \n",
    "ftext.close()\n",
    "torch.save(model.state_dict(), fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param and operator path setting\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "operator_path = './logs/nse_operator_fno_test'\n",
    "\n",
    "lr = 5e-1\n",
    "epochs = 1000\n",
    "step_size = 100\n",
    "gamma = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and model setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N0: 25, nt: 99, ny: 128, nx: 64\n"
     ]
    }
   ],
   "source": [
    "# load_data\n",
    "data_num = 0\n",
    "data, _, Cd, Cl, ang_vel = torch.load(data_path, map_location=lambda storage, loc: storage)\n",
    "data_in = data[data_num].squeeze()[0].to(device)\n",
    "data_fin = data[data_num].squeeze()[0].to(device)\n",
    "\n",
    "# data params\n",
    "ny = data.shape[2] \n",
    "nx = data.shape[3]\n",
    "s = data.shape[2] * data.shape[3]     # ny * nx\n",
    "N0 = data.shape[0]                    # num of data sets\n",
    "nt = data.shape[1] - 1                # nt\n",
    "print('N0: {}, nt: {}, ny: {}, nx: {}'.format(N0, nt, ny, nx))\n",
    "\n",
    "# load model\n",
    "load_model = FNO_ensemble(modes, modes, width, L).to(device)\n",
    "state_dict = torch.load(operator_path)\n",
    "load_model.load_state_dict(state_dict)\n",
    "load_model.eval()\n",
    "\n",
    "for param in list(load_model.parameters()):\n",
    "    param.requires_grad = False\n",
    "\n",
    "# set policy net\n",
    "# p_net = torch.nn.ModuleList([policy_net_cnn().to(device) for _ in range(nt)])\n",
    "p_net = policy_net_cnn().to(device)\n",
    "\n",
    "# training\n",
    "optimizer = torch.optim.Adam(p_net.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model without env.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start train of phase2\n",
    "ftext = open('logs/nse_control_p_test.txt', mode=\"a\", encoding=\"utf-8\")\n",
    "ftext.write(f\"phase2 | data_num: {data_num}\")\n",
    "ftext.write('N0: {}, nt: {}, ny: {}, nx: {}'.format(N0, nt, ny, nx))\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    p_net.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    ang_optim = torch.rand(nt).to(device)\n",
    "    out_nn = data_in.reshape(ny, nx, 3).to(device)\n",
    "    Cd_nn = torch.zeros(nt).to(device)\n",
    "    Cl_nn = torch.zeros(nt).to(device)\n",
    "    Cd_obs = torch.zeros(nt).to(device)\n",
    "    Cl_obs = torch.zeros(nt).to(device)\n",
    "    for i in range(nt):\n",
    "        out_nn = out_nn.reshape(1, ny, nx, 3)\n",
    "        # print('ang_optim[i]: {}'.format(ang_optim[i].size()))\n",
    "        # ang_optim[i] = p_net(out_nn.reshape(1, -1))\n",
    "        ang_optim[i] = p_net(out_nn)\n",
    "        print(ang_optim[i].min(), out_nn.max())\n",
    "        ang_nn = ang_optim[i].reshape(1, 1, 1, 1).repeat(1, ny, nx, 1)\n",
    "        # in_nn = torch.cat((out_nn.squeeze(), ang_nn), dim=-1).unsqueeze(0)\n",
    "        out_nn, Cd_nn[i], Cl_nn[i], ang_rec = load_model(out_nn, ang_nn)\n",
    "        print(f\"epoch: {epoch} | Cd_nn: {Cd_nn[i]} | Cl_nn: {Cl_nn[i]} | ang_rec: {ang_rec.item()} | i: {i}\")\n",
    "    \n",
    "\n",
    "    loss = torch.mean(Cd_nn ** 2) + 0.1 * torch.mean(Cl_nn ** 2)\n",
    "    # loss += 0.01 * torch.mean(ang_optim.squeeze() ** 2)\n",
    "    print(\"epoch: {:4}  loss: {:1.6f}  Cd_nn: {:1.6f}  Cd_obs: {:1.6f}  Cl_nn: {:1.6f}  Cl_obs: {:1.6f}  ang_optim: {:1.6f}\"\n",
    "            .format(epoch, loss, Cd_nn.mean(), Cd_obs.mean(), Cl_nn.mean(), Cl_obs.mean(), ang_optim.mean()))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # save log\n",
    "    ftext.write(\"epoch: {:4}  loss: {:1.6f}  Cd_nn: {:1.6f}  Cd_obs: {:1.6f}  Cl_nn: {:1.6f}  Cl_obs: {:1.6f}  ang_optim: {:1.6f}\\n\"\n",
    "                .format(epoch, loss, Cd_nn.mean(), Cd_obs.mean(), Cl_nn.mean(), Cl_obs.mean(), ang_optim.mean()))\n",
    "ftext.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model with env.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start train model of phase2 with env\n",
    "ftext = open('logs/phase2&env.txt', mode=\"a\", encoding=\"utf-8\")\n",
    "ftext.write(f\"phase2&env | data_num: {data_num}\")\n",
    "ftext.write('N0: {}, nt: {}, ny: {}, nx: {}'.format(N0, nt, ny, nx))\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    env.reset()\n",
    "\n",
    "    p_net.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    ang_optim = torch.rand(nt).to(device)\n",
    "    out_nn = data_in.reshape(ny, nx, 3).to(device)\n",
    "    Cd_nn = torch.zeros(nt).to(device)\n",
    "    Cl_nn = torch.zeros(nt).to(device)\n",
    "    Cd_obs = torch.zeros(nt).to(device)\n",
    "    Cl_obs = torch.zeros(nt).to(device)\n",
    "    for i in range(nt):\n",
    "        # print('ang_optim[i]: {}'.format(ang_optim[i].size()))\n",
    "        # ang_optim[i] = p_net(out_nn.reshape(1, -1))\n",
    "        ang_optim[i] = p_net[i](out_nn.reshape(1, ny, nx, 3))\n",
    "        ang_nn = ang_optim[i].reshape(1, 1, 1).repeat(ny, nx, 1)\n",
    "        in_nn = torch.cat((out_nn.squeeze(), ang_nn), dim=-1).unsqueeze(0)\n",
    "        out_nn, Cd_nn[i], Cl_nn[i] = load_model(in_nn)\n",
    "        ang_obs = ang_optim[i].to(torch.device('cpu')).detach().numpy()\n",
    "        out_obs, _, Cd_obs[i], Cl_obs[i] = env.step(ang_obs)\n",
    "        # print(f\"epoch: {epoch} | Cd_nn: {Cd_nn} | Cl_nn: {Cl_nn} | i: {i}\")\n",
    "    \n",
    "\n",
    "    loss = torch.mean(Cd_nn ** 2) + 0.1 * torch.mean(Cl_nn ** 2)\n",
    "    loss += 0.01 * torch.mean(ang_optim.squeeze() ** 2)\n",
    "    print(\"epoch: {:4}  loss: {:1.6f}  Cd_nn: {:1.6f}  Cd_obs: {:1.6f}  Cl_nn: {:1.6f}  Cl_obs: {:1.6f}  ang_optim: {:1.6f}\"\n",
    "            .format(epoch, loss, Cd_nn.mean(), Cd_obs.mean(), Cl_nn.mean(), Cl_obs.mean(), ang_optim.mean()))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # save log\n",
    "    ftext.write(\"epoch: {:4}  loss: {:1.6f}  Cd_nn: {:1.6f}  Cd_obs: {:1.6f}  Cl_nn: {:1.6f}  Cl_obs: {:1.6f}  ang_optim: {:1.6f}\"\n",
    "                .format(epoch, loss, Cd_nn.mean(), Cd_obs.mean(), Cl_nn.mean(), Cl_obs.mean(), ang_optim.mean()))\n",
    "ftext.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50d4324570b9047bb8497de954012e60daa990744e3e468319d6e91ebba54431"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
